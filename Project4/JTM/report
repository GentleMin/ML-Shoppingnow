We used three-stage training to tackle the problem.
1) First, an autoencoder is trained on the large dataset to obtain a good low-dimensional representation of the molecules. The encoder and decoder parts take the same shape, each with 4 layers, and has a 24-dimensional the bottleneck layer.
2) Next, a neural network is set up to use LUMO values as labels, and is trained on the large dataset to predict the LUMO of molecules. The encoder pretrained in step 1 is transferred to this neural network, which is then connected to a 5 layer NN predictor. Training is conducted with respect to the predictor parameters. A dropout layer of p=0.2 is included in the predictor for robustness. Despite this, oscillations are still seen in validation loss. In the end, the training result from epoch 30 is chosen as the preferred model.
3) Finally, a neural network is set up to use HOMO-LUMO gaps as labels, and is trained on the small dataset to predict the desired result. It has the same structure as the LUMO-prediction network, with the encoder and first two layers of the predictor transferred from pretrained parameters. The last three layers, which contain dropout at the bottom, are then tuned. The training is carried out with l2 regularization (equiv. implemented as weight_decay in Adam optimizer), but depending on the stepsize, the algorithm is likely to either converge to and be trapped in local minimum, or lead to oscillatory validation loss. In the end, the training result from epoch 286 is chosen as the preferred model.